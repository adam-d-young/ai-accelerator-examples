fullnameOverride: qwen2-1.5b

# Force RawDeployment mode (not serverless)
deploymentMode: RawDeployment

# Use CPU-only image instead of CUDA image
image:
  image: 'quay.io/modh/vllm'
  tag: 'v0.4.2'

# Model configuration
model:
  uri: oci://quay.io/redhat-ai-services/modelcar-catalog:qwen2-1.5b-instruct
  modelNameOverride: qwen2-1.5b
  # Remove GPU-specific arguments and add CPU-friendly ones
  args:
    - "--tensor-parallel-size=1"
    - "--device=cpu"

# CPU-only resource configuration - OVERRIDE GPU defaults
resources:
  requests:
    cpu: "2"
    memory: "4Gi"
    nvidia.com/gpu: "0"  # Explicitly override default GPU request
  limits:
    cpu: "4"
    memory: "8Gi"
    nvidia.com/gpu: "0"  # Explicitly override default GPU limit

# Remove GPU tolerations completely
tolerations: []

# Clear node selector
nodeSelector: {}

# ServingRuntime args for CPU deployment
servingRuntime:
  args:
    - --port=8080
    - --model=/mnt/models
    - --tensor-parallel-size=1