fullnameOverride: qwen2-1.5b

# Model configuration
model:
  uri: oci://quay.io/redhat-ai-services/modelcar-catalog:qwen2-1.5b-instruct
  modelNameOverride: qwen2-1.5b
  args:
    - --served-model-name=qwen2-1.5b
    - --tensor-parallel-size=1

# Force RawDeployment mode (not serverless)
deploymentMode: RawDeployment

# Override the default ServingRuntime to remove GPU dependencies
servingRuntime:
  template:
    metadata:
      annotations:
        opendatahub.io/template-display-name: "vLLM CPU ServingRuntime for KServe"
        opendatahub.io/recommended-accelerators: "[]"
    spec:
      containers:
        - name: kserve-container
          image: quay.io/modh/vllm:v0.4.2
          command:
            - python
            - -m
            - vllm.entrypoints.openai.api_server
          args:
            - --port=8080
            - --model=/mnt/models
            - --tensor-parallel-size=1
          env:
            - name: HF_HOME
              value: /tmp/hf_home

# CPU-only resource configuration - NO GPU
resources:
  limits:
    cpu: "4"
    memory: "8Gi"
  requests:
    cpu: "2"
    memory: "4Gi"

# Remove all GPU-related configurations
tolerations: []
nodeSelector: {}