fullnameOverride: qwen2-5-0-5b-instruct-cpu

model:
  mode: uri
  uri: oci://quay.io/redhat-ai-services/modelcar-catalog:qwen2.5-0.5b-instruct
  args:
    - --max-model-len=4000
    - --enable-auto-tool-choice
    - --tool-call-parser=hermes
    - --enforce-eager
    - --disable-log-stats

deploymentMode: RawDeployment

endpoint:
  externalRoute:
    enabled: false

# Use the CPU-optimized vLLM image (same as 3scale example)
image:
  image: quay.io/rh-aiservices-bu/vllm-cpu-openai-ubi9
  tag: "0.3"
  runtimeVersionOverride: 0.7.3

# Remove GPU tolerations (same as 3scale example)
tolerations: []

# CPU resources with explicit null for nvidia.com/gpu (increased for larger context)
resources:
  requests:
    cpu: '2'
    memory: 6Gi
    nvidia.com/gpu: null
  limits:
    cpu: '4'
    memory: 12Gi
    nvidia.com/gpu: null