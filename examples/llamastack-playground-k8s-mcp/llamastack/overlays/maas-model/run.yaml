# Llama Stack Configuration
version: '2'
image_name: vllm
apis:
  - agents
  - datasetio
  - eval
  - inference
  - safety
  - scoring
  - tool_runtime
  - vector_io
models:
  - metadata: {}
    model_id: llama-4-scout-17b-16e-w4a16
    provider_id: llama-4-scout-17b-16e-w4a16-maas
    provider_model_id: llama-4-scout-17b-16e-w4a16
    model_type: llm
providers:
  scoring:
    - provider_id: basic
      provider_type: inline::basic
      config: {}
    - provider_id: llm-as-judge
      provider_type: inline::llm-as-judge
      config: {}
  agents:
    - provider_id: meta-reference
      provider_type: inline::meta-reference
      config:
        persistence_store:
          type: sqlite
          db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/starter}/agents_store.db
        responses_store:
          type: sqlite
          db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/starter}/responses_store.db
  inference:
    - provider_id: llama-4-scout-17b-16e-w4a16-maas
      provider_type: "remote::vllm"
      config:
        url: "https://llama-4-scout-17b-16e-w4a16-maas-apicast-production.apps.prod.rhoai.rh-aiservices-bu.com:443/v1"
        max_tokens: 110000
        api_token: ${env.LLAMA_4_SCOUT_17B_16E_W4A16_API_TOKEN}
        tls_verify: true
    - provider_id: sentence-transformers
      provider_type: inline::sentence-transformers
  tool_runtime:
    - provider_id: model-context-protocol
      provider_type: remote::model-context-protocol
      config: {}
tool_groups:
  - toolgroup_id: mcp::openshift
    provider_id: model-context-protocol
    mcp_endpoint:
      uri: http://kubernetes-mcp.ai-agent-mcp.svc.cluster.local:8080/sse
server:
  port: 8321
